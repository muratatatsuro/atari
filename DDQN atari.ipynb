{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "from collections import deque\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dense\n",
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "ENV_NAME='Breakout-v0'\n",
    "#今回は画像データで深層強化学習を進める\n",
    "FRAME_WIDTH=84\n",
    "FRAME_HEIGHT=84\n",
    "\n",
    "#学習回数\n",
    "MAX_EPISODES=12000\n",
    "STATE_LENGTH=4#networkに入れるフレーム数\n",
    "GAMMA=0.99#割引率\n",
    "EXPLORATION_STEPS=10000#一回学習当たりのステップ数\n",
    "\n",
    "INITIAL_EPSILON=1.0#ε-greedyの最初のε\n",
    "FINAL_RPSION=0.1\n",
    "\n",
    "INITIAL_REPLAY_SIZE=2000#学習を始める前に記憶メモリから取ってくるメモリの容量\n",
    "NUM_REPLAT_MEMORY=20000#メモリの容量\n",
    "BATCH_SIZE=32\n",
    "\n",
    "TARGET_UPDATE_INTERVAL=20#TARGET_NETWORKを更新する間隔\n",
    "\n",
    "ACTION_INTERVAL=4\n",
    "TRAIN_INTERVAL=4#agentはupdateするときに四つの行動をとる\n",
    "\n",
    "LEARNING_RATE=0.0001#RMSpropの学習率\n",
    "MOMENTUM=0.95\n",
    "MIN_GRAD=0.01\n",
    "\n",
    "SAVE_INTERVAL=3000#networkを保存する回数\n",
    "NO_OP_STEPS=30#agentが何もしないという行動の最大数\n",
    "\n",
    "LOAD_NETWORK=False#学習したモデルをロードするかしないか\n",
    "\n",
    "TRAIN=True\n",
    "\n",
    "SAVE_NETWORK_PATH='saved_network/'+ENV_NAME\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,num_actions):\n",
    "        self.num_actions=num_actions\n",
    "        self.epsilon=INITIAL_EPSILON#行動選択\n",
    "        self.epsilon_step=(FINAL_EPSILON-INITIAL_EPSILON)/EXPLORATION_STEPS\n",
    "        self.t=0\n",
    "        self.repeated_action=0#一つ前の行動のリスト\n",
    "        \n",
    "        #学習のために必要なパラメタ\n",
    "        self.total_reward=0#報酬合計和\n",
    "        self.total_q_max=0#最大価値関数の合計\n",
    "        self.total_loss=0#Q関数の合計損失\n",
    "        self,episode=0#学習回数(now)\n",
    "        self.duratuin=0#?\n",
    "        \n",
    "        #学習時間を計測するため　初期値\n",
    "        self.start=0\n",
    "        \n",
    "        #memory\n",
    "        self.replay_memory=deque()\n",
    "        \n",
    "        #q_networkの構築\n",
    "        \n",
    "        #main\n",
    "        self.s,self.main_q_values,main_q_network=self.net()\n",
    "        main_q_network_weights=main_q_network.trainable_weights#main_q_networkの初期パラメタ\n",
    "        \n",
    "        #target\n",
    "        self.st,self.target_q_values,target_network=self.net()\n",
    "        target_q_network_weights=target_q_network.trainable_weights\n",
    "        \n",
    "        #main_q_networkのパラメータで更新するためのリスト\n",
    "        self.update_target_network=[target_network_weights[i].assign(main_q_network_weights[i]) for i in range(len(target_q_network_weights))]\n",
    "        \n",
    "        #最適化アルゴリズム指標\n",
    "        \"\"\"a:actionのvector,y:教師信号,loss,optimizer\"\"\"\n",
    "        self.a,self.y,self.loss,self.optimizer=self.train_op(main_q_network_weights)\n",
    "        \n",
    "        \n",
    "        #tensorflowで実行\n",
    "        self.sess=tf.InteractiveSession()#with文がいらない\n",
    "        self.saver=tf.train.Saver(main_q_network_weights)#保存\n",
    "        \n",
    "        #listの作成\n",
    "        if not os.path.exists(SAVE_NETWORK_PATH):\n",
    "            os.mkdir(SAVE_NETWORK_PATH)\n",
    "            \n",
    "        #学習パラメータの初期化\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        #load network すでに学習したモデルを用いて環境実行を行う\n",
    "        if LOAD_NETWORK:\n",
    "            self.load_network()\n",
    "            \n",
    "            \n",
    "        #target_q_network_weightsを初期化\n",
    "        self.sess.run(self.update_target_network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def net(self):\n",
    "        \"\"\"入力データ、q値、深層ネットワークモデルを返す\"\"\"\n",
    "        model=Sequential()\n",
    "        model.add(Conv2D(32,8,strides=(4,4),activation='relu',input_shape=(STATE_LENGTH,FRAME_WIDTH,FRAME_HEIGHT)))\n",
    "        model.add(Conv2D(64,4,strides=(2,2),activation='relu'))\n",
    "        model.add(Conv2D(64,3,strides=(1,1),activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation='relu'))\n",
    "        model.add(Dense(self,num_actions))\n",
    "        \n",
    "        #入力する画像データ\n",
    "        img=tf.placeholder(dtype='float32',shape=[None,STATE_LENGTH,FRAME_WIDTH,FRAME_HEIGHT])\n",
    "        #データをモデルに入れて出てくるのがQ値\n",
    "        q_values=model(img)\n",
    "        \n",
    "        return img,q_values,model\n",
    "    \n",
    "    \n",
    "    def train_op(self,q_network_weights):\n",
    "        a=tf.placeholder(tf.int64,[None])#行動\n",
    "        y=tf.placeholder(tf.float32,[None])#教師信号\n",
    "        \n",
    "        #onehot vector化\n",
    "        a_onehot=tf.one_hot(a,self.num_actions,1.0,0.0)#shape=(batch_size,num_actions)\n",
    "        q_value=tf.reduce_sum(tf.multiply(self.main_q_values,a_onehot),reduction_indices=1)#shape=(batch_size,)\n",
    "        \n",
    "        #正則化も含めて\n",
    "        error=tf.abs(q_value-y)\n",
    "        quardratic=tf.clip_by_value(error,0.0,1.0)#errorがこの値に収束するように設定するためのclip_by_value\n",
    "        loss=tf.reduce_mean(0.5*tf.square(quardratic)+(error-quardratic))\n",
    "        \n",
    "        optimizer=tf.train.RMSPropOptimizer(LEARNING_RATE,momentum=MOMENTUM,epsilon=MIN_GRAD)\n",
    "        optimizer=optimizer.minimize(loss,var_list=q_network_weights)#更新するパラメータq_network_weights\n",
    "        \n",
    "        return a,y,loss,optimizer\n",
    "    \n",
    "    \n",
    "    def get_action(self,state):\n",
    "        action=self.repeated_action\n",
    "        \n",
    "        if self.t%ACTION_INTERVAL==0:\n",
    "            if self.epsilon>=np.random.rand() or self.t<ACTION_INTERVAL:\n",
    "                action=random.randrange(self.num_actions)#行動の中からランダムに選ぶ\n",
    "            else:\n",
    "                #推論モードのq_valueで最大のものを選ぶQ(s(t+1),a)\n",
    "                \n",
    "                #stateの入力データが必要\n",
    "                action=np.argmax(self.main_q_values.eval(feed_dict={self.s:[np.float32(state/255)]}))\n",
    "                \n",
    "            self.repeated_action=action\n",
    "            \n",
    "        #epsilonの更新\n",
    "        if self.epsilon>FINAL_EPSILON and self.t>=INITIALIZE_REPLAY_SIZE:\n",
    "            self.epsilon-=self.epsilon_step\n",
    "            \n",
    "            \n",
    "        return action\n",
    "    \n",
    "    #観測(observation)を状態(state)に変える\n",
    "    def get_initial_state(self,observation,before_observation):\n",
    "        preprocessed_observation=np.maximum(observation,before_observation)\n",
    "        #カラー画像をグレースケールに\n",
    "        preprocessed_observation=cv2,cvtColor(preprocessed_observation,cv2.COLOR_RGB2GRAY)\n",
    "        #resize\n",
    "        preprocessed_observation=cv2.resize(preprocessed_observation,(FRANE_WIDTH,FRAME_HEIGHT))\n",
    "        \n",
    "        preprocessed_observation=np.uint8(preprocessed_observation*255)\n",
    "        \n",
    "        state=[preprocessed_observation for _ in range(STATE_LENGTH)]#一度に入れる状態の数のリストを作成\n",
    "        \n",
    "        state=np.stack(state,axis=0)#行方向に結合\n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def run(self,state,action,reward,observation_next,terminal):\n",
    "        \"\"\"次の状態を返すための操作を行う関数\n",
    "        　 環境(env)の実行(step(action))によって生成されるデータは、observation_next,reward,terminal,(info)\n",
    "           observation_nextは次の状態、rewardは報酬、terminalは学習終了判定\n",
    "           env runによりobservation_next,reward,terminalを取得\"\"\"\n",
    "        next_state=np.append(state[1:,:,:],observation_next,axis=0)#行方向に足していく\n",
    "        \n",
    "        #報酬の渡し方 符号によって決定\n",
    "        reward=np.sign(reward)\n",
    "        \n",
    "        #memoryに保存 transitionsともいう\n",
    "        self.replay_memory.append((state,action,reward,next_state,terminal))\n",
    "        \n",
    "        #memoryの容量をオーバーしたとき\n",
    "        if len(self.replay_memory)> NUM_REPLAY_MEMORY:\n",
    "            self.replay_memory.popleft()#最初の方のデータは捨てる\n",
    "         \n",
    "        #Q関数をupdate\n",
    "        if self.t>= INITIALIZE_REPLAY_SIZE:\n",
    "            \n",
    "            #update_q_network\n",
    "            if self.t%TRAIN_INTERVAL==0:\n",
    "                self.replay()\n",
    "            \n",
    "            #update_target_network\n",
    "            if self.t%TARGET_UPDATE_INTERVAL==0:\n",
    "                self.sess.run(self.update_target_network)\n",
    "                \n",
    "            if self.t%SAVE_INTERVAL==0:\n",
    "                save_path=self.saver.save(self.sess,SAVE_NETWORK_PATH+'/'+ENV_NAME,global_step=self.t)\n",
    "                print('success saved')\n",
    "                \n",
    "        self.total_reward+=reward\n",
    "        self.total_q_max=np.max(self.main_q_values.eval(feed_dict={self.s:[np.float32(state/255)]}))\n",
    "        self.duration+=1\n",
    "        \n",
    "        \n",
    "        #終了した場合の処理\n",
    "        if terminal:\n",
    "            \n",
    "            elapsed=time.time()-self.start\n",
    "            if self.t<INITIALIZE_REPLAY_SIZE:\n",
    "                mode='random'\n",
    "            elif INITIALIZE_REPLAY_SIZE<=self.t<INITIALIZE_REPLAY_SIZE+EXPLORATION_STEPS:\n",
    "                mode='exploration'\n",
    "            else:\n",
    "                mode='exploit'\n",
    "            #終了したら合計値は初期化\n",
    "            print('episode:{0:5d}/timestep:{1:8d}/duration{2:5d}/epsilon{3:3f}/total_reward{4:3.0f}/avg_max_q{5:2.4f}/avg_loss{6:.5f}/mode{7}/step_per_second{8:.1f}').format((\n",
    "            self.episode+1,self.t,self.duration,self.epsilon,self.total_reward,self.total_q_max/float(self.duration),self.total_loss/(float(self.duration)/float(self.TRAIN_INTERVAL)),mode,self.duration/elapsed))\n",
    "            self.total_reward=0\n",
    "            self.total_q_max=0\n",
    "            self.total_loss=0\n",
    "            self.duration=0\n",
    "            self.episode+=1\n",
    "        self.t+=1\n",
    "        \n",
    "        return next_state\n",
    "                \n",
    "                \n",
    "                \n",
    "    def replay(self):\n",
    "        #メモリから学習データを作成　バッチ学習\n",
    "        state_batch=[]#状態s(t)のbatch\n",
    "        action_batch=[]#行動a(t)のbatch\n",
    "        reward_batch=[]#reward batch\n",
    "        next_state_batch=[]#s(t+1)のbatch\n",
    "        terminal_batch=[]#終了判定のbatch\n",
    "        y_batch=[]#教師データのbatch\n",
    "        \n",
    "        \n",
    "        #メモリからbatch_sizeだけデータを持ってくる\n",
    "        #memory=[state,action,reward,next_state,terminal]\n",
    "        batch_data=random.sample(self.replay_memory,BATCH_SIZE)\n",
    "        \n",
    "        for data in batch_data:\n",
    "            #shape=(batch_size,4,:,:)\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "            \n",
    "            \n",
    "        #terminal_batchを数値化\n",
    "        terminal_batch=[1 if batch==True else 0 for batch in terminal_batch ]\n",
    "        \n",
    "        #target_q_networkのvalue\n",
    "        target_q_values_batch=self.target_q_values.eval(feed_dict={self.st:np.float32(np.array(next_state_batch)/255)})\n",
    "        \n",
    "        ##DDQN\n",
    "        actions=np.argmax(self.main_q_values.eval(feed_dict={self.s:np.float32(np.array(next_state_batch)/255)}),axis=1)\n",
    "        \n",
    "        #推定した行動からtarget_q_valuesを変化させる\n",
    "        target_q_values_batch=np.array([target_q_values_batch[i][action] for i,action in enumerate(actions)])\n",
    "        \n",
    "        #教師信号のバッチ\n",
    "        y_batch=reward_batch+(1-terminal_batch)*GAMMA*target_q_values_batch\n",
    "        \n",
    "        #学習\n",
    "        loss,_=self.sess.run([self.loss,self.optimizer],feed_dict={self.s:np.float32(np.array(state_batch)/255),self.a:action_batch,self.y:y_batch})\n",
    "        \n",
    "        self.total_loss+=loss\n",
    "        \n",
    "        \n",
    "    def load_network(self):\n",
    "        checkpoint=tf.train.get_checkpoint_state(SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_checkpoint_path:\n",
    "            self.saver.restore(self.sess,checkpoint.model_checkpoint_path)\n",
    "            print('success load')\n",
    "        else:\n",
    "            print('training new network')\n",
    "            \n",
    "            \n",
    "    \n",
    "            \n",
    "            \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#データの前処理\n",
    "def preprocessing(observation,before_observation):\n",
    "    preprocessed_observation=np.maximum(observation,before_observation)\n",
    "    preprocessed_observation=cv2.cvtColor(preprocessed_observation,cv2.COLOR_RGB2GRAY)\n",
    "    preprocessed_observation=cv2.resize(preprocessed_observation,(FRAME_WIDTH,FRAME_HEIGHT))\n",
    "    preprocesed_observation=np.uint8(preprocessed_observation/255)\n",
    "    return np.reshape(preprocessed_observation,(1,FRAME_WIDTH,FRAME_HEIGHT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.env=gym.make(ENV_NAME)\n",
    "        #環境を定義したときに行動の個数と状態の個数は把握しておく必要性あり\n",
    "        num_actions=self.env.action_space.n\n",
    "        self.agent=Agent(num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        if TRAIN:\n",
    "            for _ in range(MAX_EPISODES):\n",
    "                terminal=False\n",
    "                observation=self.env.reset()\n",
    "                \n",
    "                #ここでランダムに初期化をしておく\n",
    "                for _ in range(random.randint(0,NO_OP_STEPS=30)):\n",
    "                    before_observation=observation\n",
    "                    observation,_,_,_=self.env.step(0)#0は行動を起こさないということ\n",
    "                state=preprocessing(observation,before_observation)\n",
    "                self,agent.start=time.time()\n",
    "                \n",
    "                while not terminal:\n",
    "                    before_observation=observation\n",
    "                    action=self.agent.get_action(state)\n",
    "                    observation,reward,terminal,_=self.env.step(action)\n",
    "                    self.env.render()\n",
    "                    processed_observation=preprocessing(observation,before_observation)\n",
    "                    state=self.agent.run(state,action,reward,processed_observation,terminal)\n",
    "                    \n",
    "        #test mode        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game_env=Environment()\n",
    "game_env.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
