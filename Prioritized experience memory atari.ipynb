{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'DDQNの中でTD_error_memoryを作っておいてこれをもとにmemoryの優先順位をつける'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"DDQNの中でTD_error_memoryを作っておいてこれをもとにmemoryの優先順位をつける\n",
    "\n",
    "実装途中のものは空白を開けておく\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 5, 6, 2, 6, 7, 8, 9, 4, 4, 6, 7, 8, 3, 9, 4]\n"
     ]
    }
   ],
   "source": [
    "a=[3,5,6,2,6,7,8,9,4]\n",
    "b=[4,6,7,8,3,9,4]\n",
    "a.extend(b)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import keras\n",
    "import gym\n",
    "import time\n",
    "from collections import deque\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D,MaxPooling2D,Dense,Dropout,Flatten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters\n",
    "ENV_NAME='Breakout-v0'\n",
    "\n",
    "FRAME_WIDTH=84\n",
    "FRAME_HEIGHT=84\n",
    "STATE_LENGTH=4#一度に入れるフレーム数\n",
    "MAX_EPISODES=1200\n",
    "GAMMA=0.99#割引率\n",
    "EXPLORATION_STEPS=100000#一学習当たりの学習ステップ数\n",
    "INITIAL_EPSILON=1.0#ε-greedy\n",
    "FINAL_EPSILON=0.1\n",
    "\n",
    "NUM_REPLAY_MEMORY=200000#メモリ容量\n",
    "BATCH_SIZE=32\n",
    "TARGET_UPDATE_INTERVAL=1000#target_q_networkを更新する頻度\n",
    "ACTION_INTERVAL=4\n",
    "TRAIN_INTERVAL=4\n",
    "\n",
    "LEARNING_RATE=0.0001#RMSPropの学習率\n",
    "MOMNTUM=0.95\n",
    "MIN_GRAD=0.01\n",
    "\n",
    "SAVE_INTERVAL=300#保存頻度\n",
    "NO_OP_STEPS=30#初期化の回数\n",
    "\n",
    "LOAD_NETWORK=False#学習済みデータをloadするか\n",
    "TRAIN=True\n",
    "\n",
    "SAVE_NETWORK_PATH='savenetwork/'+ENV_NAME\n",
    "\n",
    "INITUAL_BETA=0.4#IS weight?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDerrorMemory():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.capacity=NUM_REPLAY_MEMORY\n",
    "        self.td_error_list=[]\n",
    "        self.indexes=[]\n",
    "        self.memory=[self.td_error_list,self.indexes]\n",
    "        self.index=0\n",
    "        \n",
    "    def create_td_error(self,td_error):\n",
    "        if len(self.td_error_list)<self.capacity:\n",
    "            self.td_error_list.append(None)\n",
    "            \n",
    "        self.td_error_list[index]=td_error\n",
    "        self.index=(self.index+1)%self.capacity\n",
    "    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "    def prioritized_indexes(self,batch_size):\n",
    "        \n",
    "        #TD誤差の和を出す\n",
    "        sum_absolute_td_error=np.sum(np.absolute(self.td_error_list))\n",
    "        sum_absolute_td_error+=0.0001*len(self.td_error_list)#計算の安定性のために微小量を足す\n",
    "        \n",
    "        rand=np.random.uniform(0,sum_absolute_td_error,BATCH_SIZE)\n",
    "        rand=np.sort(rand)\n",
    "        \n",
    "        idx=0\n",
    "        indexes=[]\n",
    "        tmp_sum_absolute_td_error=0#現在のtd_error\n",
    "        \n",
    "        for r in rand:\n",
    "            while tmp_sum_absolute_td_error<r:\n",
    "                tmp_sum_absolute_td_error+=(abs(self.td_error_list[idx])+0.0001)\n",
    "                idx+=1\n",
    "                \n",
    "            if idx>=len(self.td_error_list):\n",
    "                idx=len(self.td_error_list)-1\n",
    "            indexes.append(idx)\n",
    "        self.indexes=indexes\n",
    "            \n",
    "        return indexes\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self,num_actions):\n",
    "        self.num_actions=num_actions\n",
    "        self.epsilon=INITIAL_EPSILON\n",
    "        self.epsilon_step=(INITIAL_EPSILON-FINAL_EPSILON)/EXPLORATION_STEPS\n",
    "        self.repeated_action=0\n",
    "        self.t=0\n",
    "        \n",
    "        \n",
    "        #parameters\n",
    "        self.total_reward=0\n",
    "        self.total_q_max=0\n",
    "        self.total_loss=0\n",
    "        self.duration=0\n",
    "        self.episode=0\n",
    "        \n",
    "        self.start=0#時間を求める\n",
    "        \n",
    "        #memory\n",
    "        self.memory=deque()#local buffer\n",
    "        self.td_error_memory=TDerrorMemory()#replay\n",
    "        \n",
    "        #network作成\n",
    "        self.s,self.q_values,q_network=self.net()\n",
    "        q_network_weights=q_network.trainable_weights#重みの初期パラメータ(main_q)\n",
    "        \n",
    "        self.st,self.target_q_values,target_q_network=self.net()\n",
    "        target_q_network_weights=target_q_network.trainable_weights\n",
    "        \n",
    "        #重みを更新していくためのtaget_q_network_weights list\n",
    "        self.update_target_network=[target_q_network_weights[i].assign(q_network_weights[i]) for i in range(len(target_q_network_weights))]\n",
    "        \n",
    "        #最適化手法\n",
    "        \"\"\"行動、教師信号、TD誤差、損失、最適化アルゴを返す\"\"\"\n",
    "        self.a,self.y,self.error,self.loss,self.optimizer=self.train_op(q_network_weights)\n",
    "        \n",
    "        #tensorflowで実行するためのインスタンス\n",
    "        self.sess=tf.InteractiveSession()\n",
    "        self.saver=tf.train.Saver(q_network_weights)\n",
    "        \n",
    "        \n",
    "        if not os,path.exists(SAVE_NETWORK_PATH):\n",
    "            os.mkdir(SAVE_NETWORK_PATH)\n",
    "            \n",
    "        #全てのパラメタの初期化\n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        if LOAD_NETWORK:\n",
    "            self.load_network()\n",
    "            \n",
    "        \n",
    "        #target_networkの初期化\n",
    "        self.sess.run(self.update_target_network)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    #neural networkの構造    \n",
    "    def net(self):\n",
    "        model=Sequential()\n",
    "        model.add(Conv2D(32,8,(4,4),activation='relu',input_shape=(STATE_LENGTH,FRAME_WIDTH,FRAME_HEIGHT)))\n",
    "        model.add(Conv2D(64,4,(2,2),activation='relu'))\n",
    "        model.add(Conv2D(64,3,(1,1),activation='relu'))\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512,activation='relu'))\n",
    "        model.add(Dense(self.num_actions))\n",
    "        \n",
    "        s=tf.placeholder('float32',[None,STATE_LENGTH,FRAME_WIDTH,FRAME_HEIGHT])\n",
    "        q_values=model(s)\n",
    "        \n",
    "        return s,q_values,model\n",
    "    \n",
    "    #最適化手法 重みを更新していく\n",
    "    def train_op(self,q_network_weights):\n",
    "        \"\"\"ここで返されるerrorがTD_error\"\"\"\n",
    "        a=tf.placeholder(tf.int64,[None])#行動\n",
    "        y=tf.placeholder(tf.float32,[None])#教師信号\n",
    "        \n",
    "        \n",
    "        \n",
    "        #行動をベクトル化\n",
    "        a_onehot=tf.one_hot(a,self.num_actions,1.0,0.0)\n",
    "        \n",
    "        q_value=tf.reduce_sum(tf.multiply(self.q_values,a_onehot),reduction_indices=1)\n",
    "        \n",
    "        #loss\n",
    "        error=tf.abs(y-q_value)\n",
    "        quardratic_part=tf.clip_by_value(error,0.0,1.0)#error_isの値が-1,1になるように\n",
    "        linear_part=error-quardratic_part\n",
    "        \n",
    "        loss=tf.reduce_mean(0.5*tf.square(quardratic_part)+linear_part)\n",
    "        \n",
    "        #最適化\n",
    "        optimizer=tf.train.RMSPropOptimizer(LEARNING_RATE,momentum=MOMENTUM,epsilon=MIN_GRAD)\n",
    "        optimizer=optimizer.minimize(loss,var_list=q_network_weights)\n",
    "        \n",
    "        return a,y,error,loss,optimizer\n",
    "    \n",
    "    #ε-greedyで行動選択\n",
    "    def get_action(self,state):\n",
    "        action=self.repeated_action\n",
    "        \n",
    "        if self.t%ACTION_INTERVAL==0:\n",
    "            if self.epsilon>=np.random.rand() and self.t<ACTION_INTERVAL:\n",
    "                action=random.tandrange(self.num_actions)\n",
    "                \n",
    "            else:\n",
    "                action=np.argmax(self.q_values.eval(feed_dict={self.s:[np.float32(state/255)]}))\n",
    "                \n",
    "            self.repeated_action=action\n",
    "            \n",
    "        if epsilon>FINAL_epsilon and self.t>=INITIALIZE_REPLAY_SIZE:\n",
    "            self.epsilon-=self.epsilon_step\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    \n",
    "    #観測をstateに変換\n",
    "    def get_initial_state(self,observation,before_observation):\n",
    "        preprocessed_observation=np.maximum(observation,before_observation)\n",
    "        #グレースケールに\n",
    "        preprocessed_observation=cv2.cvtColor(preprocessed_observation,cv2.COLOR_RGB2GRAY)\n",
    "        #リサイズ\n",
    "        preprocessed_observation=cv2.resize(preprocessed_observation,(FRAME_WIDTH,FRAME_HEIGHT))\n",
    "        #8ビット化\n",
    "        preprocessed_observation=np.uint8(preprocessed_observation*255)\n",
    "        \n",
    "        state=[preprocessed_observation for _ in range(STATE_LENGTH)]\n",
    "        \n",
    "        return np.stack(state,axis=0)\n",
    "    \n",
    "    \n",
    "    #まだ実装途中\n",
    "    def run(self,state,action,reward,terminal,observation_next):\n",
    "        \"\"\"環境の実行(env.step)で返されるのはobservation_next,reward,terminal,info\"\"\"\n",
    "        next_state=np.append(state[1:,:,:],observation_next,axis=0)\n",
    "        \n",
    "        #報酬の定義 positive + negative - ,他は0\n",
    "        reward=np.sign(reward)\n",
    "        \n",
    "        #memoryに状態を保存する\n",
    "        self.memory.append((state,action,reward,next_state,terminal))\n",
    "        if len(self.memory)> NUM_REPLAY_MEMORY:\n",
    "            self.memory.popleft()\n",
    "            \n",
    "            #学習のインターバル\n",
    "            if self.t%TRAIN_INTERVAL==0:\n",
    "                self.replay()\n",
    "             \n",
    "            #target_network_weightsの更新\n",
    "            if self.t%TARGET_UPDATE_INTERVAL==0:\n",
    "                self.sess.run(update_target_network)\n",
    "                \n",
    "            #学習の結果の保存\n",
    "            if self.t%SAVE_INTERVAL==0:\n",
    "                self.saver.save(self.sess,SAVE_NETWORK_PATH+'/'+ENV_NAME,global_step(self.t))\n",
    "                \n",
    "        self.total_reward+=reward\n",
    "        self.total_q_max+=np.max(self.q_network.eval(feed_dict={self.s:[np.float32((state)/255)]}))\n",
    "        self.duration+=1\n",
    "        \n",
    "        #終了したときの処理\n",
    "        if terminal:\n",
    "            elapsed=time.time()-self.start\n",
    "            if self.t<INITIALIZE_REPLAY_SIZE:\n",
    "                mode='random'\n",
    "            elif INITIALIZE_REPLAY_SIZE<=self.t<INITIALIZE_REPLAY_SIZE+EXPLORATION_STEP:\n",
    "                mode='exploration'\n",
    "            elif INITIALIZE_REPLAY_SIZE+EXPLORATION_STEP<=self.t:\n",
    "                mode='exploit'\n",
    "                \n",
    "            print('episode:{0:5d}/timestep:{1:8d}/duration:{2:5d}/epsilon:{3:.3f}/total_reward:{4:3.0f}/avg_max_q:{5:2.4f}/avg_loss:{6:.5f}/mode:{7}/step_per_second{8:.1f}'.format(\n",
    "            self.episode+1,self.t,self.duration,self.epsilon,self.total_rewrad,self.total_max_q/float(self.duration),self.total_loss/(float(self.duration)/float(TRAIN_INTERVAL)),mode,self.duration/elapsed))\n",
    "            \n",
    "            #評価値のリセット\n",
    "            self.total_reward=0\n",
    "            self.total_q_max=0\n",
    "            self.total_loss=0\n",
    "            self.duration=0\n",
    "            self.episode+=1\n",
    "            \n",
    "        self.t=1\n",
    "        \n",
    "        return next_state\n",
    "                \n",
    "                \n",
    "    def replay(self):\n",
    "        action_batch=[]\n",
    "        state_batch=[]\n",
    "        reward_batch=[]\n",
    "        next_state_batch=[]\n",
    "        terminal_batch=[]\n",
    "        \n",
    "        \n",
    "        #td_error_memoryによってindexesを返す\n",
    "        indexes=self.td_error_memory.prioritized_indexes(BATCH_SIZE)\n",
    "        minibatch=self.memory[indexes]\n",
    "        \n",
    "        for data in minibatch:\n",
    "            action_batch.append(data[0])\n",
    "            state_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "            \n",
    "        #terminal_batch 数値化\n",
    "        terminal_batch=[1 if tr==True else 0 for tr in terminal_batch]\n",
    "        \n",
    "        #target_q_valueのバッチを作成\n",
    "        target_q_values_batch=self.target_q_values.eval(feed_dict={self.st:np.float32(np.array(next_state_batch)/255)})\n",
    "        \n",
    "        #DDQN\n",
    "        actions=np.argmax(self.q_values.eval(feed_dict={self.s:np.float32(np.array(next_state_batch)/255)}),axis=1)\n",
    "        target_q_values_batch=[target_q_values_batch[i][action] for i,action in enumerate(actions)]\n",
    "        \n",
    "        y_batch=reward_batch+(1-terminal_batch)*GAMMA*target_q_values_batch\n",
    "        \n",
    "        \n",
    "        #error_batch    \n",
    "        error_batch=self.error.eval(feed_dict={self.s:np.float32(np.array(state_batch)/255),self.a:action_batch,self.y:y_batch})\n",
    "        \n",
    "        #td_error_memoryの更新\n",
    "        self.td_error_memory.memory[0].extend(error_batch)\n",
    "        \n",
    "        loss,_=self.sess.run([self.loss,self.optimizer],feed_dict={self.s:np.float32(np.array(state_batch)/255),self.a:action_batch,self.y:y_batch})\n",
    "        \n",
    "        self.total_loss+=loss\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def load_network(self):\n",
    "        checkpoint=tf.train.get_checkpoint_path(SAVE_NETWORK_PATH)\n",
    "        if checkpoint and checkpoint.model_chekpoint_path:\n",
    "            self.saver.restore(self.sess,checkpoint.model_checkpoint_path)\n",
    "            print('successfully loaded')\n",
    "            \n",
    "        else:\n",
    "            print('training new network')\n",
    "            \n",
    "############################ここでclass Agent()終わり#############################\n",
    "\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(observation,before_observation):\n",
    "    preprocessing_observation=np.maximum(observation,before_observation)\n",
    "    preprocessing_observation=cv2.cvtColor(preprocessing_observation,cv2.COLOR_RGB2GRAY)\n",
    "    preprocessing_observation=cv2.resize(preprocessing_observation,(FRAME_WIDTH,FRAME_HEIGHT))\n",
    "    preprocessing_observatioFRAME_WIDTH,FRAME_HEIGHTn=np.uint8(preprocessing_observation*255)\n",
    "    return np.reshape(preprocessing,(1,FRAME_WIDTH,FRAME_HEIGHT))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self):\n",
    "        self.env=gym.make(ENV_NAME)\n",
    "        num_actions=self.env.action_space.n\n",
    "        self.agent=Agent(num_actions)\n",
    "        \n",
    "    def run(self):\n",
    "        if TRAIN:\n",
    "            terminal=False\n",
    "            obsrvation=self.env.reset()\n",
    "            \n",
    "            #最初のランダム初期化\n",
    "            for _ in range(random.randint(0,NO_OP_STEPS)):\n",
    "                before_observation=observation\n",
    "                observation,_,_,_=self.env.step(0)\n",
    "            state=self.agent.get_initial_state(before_observation,observation)\n",
    "            self.agent.start=time.time()\n",
    "            \n",
    "            #学習ループの開始\n",
    "            while not terminal:\n",
    "                before_observation=observation\n",
    "                action=self.agent.get_action(state)\n",
    "                \n",
    "                observation,reward,terminal,_=self.env.step(action)\n",
    "                self.env.render()\n",
    "                processed_observation=preprocessing(observation,before_observation)\n",
    "                state=self.agent.run(state,action,reward,terminal,processed_observation)\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#実行\n",
    "game_env=Environment()\n",
    "game_env.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
